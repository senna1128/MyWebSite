---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Semiparametric Nonlinear Bipartite Graph Representation Learning with Provable
  Guarantees
subtitle: ''
summary: ''
authors:
- Sen Na
- Yuwei Luo
- Zhuoran Yang
- Zhaoran Wang
- Mladen Kolar
tags: [stat.ML,cs.LG]
categories: []
date: '2020-05-01'
lastmod: 2023-02-28T14:15:07-08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-28T22:15:07.062896Z'
publication_types:
- '2'
abstract: 'Graph representation learning is a ubiquitous task in machine learning where the goal is to embed each vertex into a low-dimensional vector space. We consider the bipartite graph and formalize its representation learning problem as a statistical estimation problem of parameters in a semiparametric exponential family distribution: the bipartite graph is assumed to be generated by a semiparametric exponential family distribution, whose parametric component is given by the proximity of outputs of two one-layer neural networks that take high-dimensional features as inputs, while nonparametric (nuisance) component is the base measure. In this setting, the representation learning problem is equivalent to recovering the weight matrices, and the main challenges of estimation arise from the nonlinearity of activation functions and the nonparametric nuisance component of the distribution. To overcome these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and show that the proposed objective is strongly convex in a neighborhood around the ground truth, so that a gradient descent-based method achieves linear convergence rate. Moreover, we prove that the sample complexity of the problem is linear in dimensions (up to logarithmic factors), which is consistent with parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, which is validated in extensive experiments.'
publication: '*International Conference on Machine Learning*'
links:
- name: URL
  url: http://proceedings.mlr.press/v119/na20a.html
- name: arXiv
  url: https://arxiv.org/abs/2003.01013
---
