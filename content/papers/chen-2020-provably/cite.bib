@article{Chen2020Provably,
 abstract = {Training a classifier under non-convex constraints has gotten increasing attention in the machine learning community thanks to its wide range of applications such as algorithmic fairness and class-imbalanced classification. However, several recent works addressing non-convex constraints have only focused on simple models such as logistic regression or support vector machines. Neural networks, one of the most popular models for classification nowadays, are precluded and lack theoretical guarantees. In this work, we show that overparameterized neural networks could achieve a near-optimal and near-feasible solution of non-convex constrained optimization problems via the project stochastic gradient descent. Our key ingredient is the no-regret analysis of online learning for neural networks in the overparameterization regime, which may be of independent interest in online learning applications.},
 archiveprefix = {arXiv},
 author = {You-Lin Chen and Zhaoran Wang and Mladen Kolar},
 doi = {10.1214/22-EJS2036},
 eprint = {2012.15274},
 file = {:http\://arxiv.org/pdf/2012.15274v1:PDF},
 journal = {Electronic Journal of Statistics},
 keywords = {stat.ML, cs.LG, math.OC, neural tangent kernel, non-convex constrained optimization, online learning with non-convex losses},
 number = {2},
 pages = {5812 -- 5851},
 primaryclass = {stat.ML},
 publisher = {Institute of Mathematical Statistics and Bernoulli Society},
 title = {Provably training overparameterized neural network classifiers with non-convex constraints},
 url = {https://doi.org/10.1214/22-EJS2036},
 volume = {16},
 year = {2022}
}

